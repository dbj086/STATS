{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMncQrrZZ4r4ADnO7YQmF0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbj086/STATS/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##THEORY QUESTION##\n",
        "##Q1)What is Logistic Regression, and how does it differ from Linear Regression ?\n",
        "\n",
        "Ans:**Logistic Regression** is used for classification tasks, predicting probabilities (between 0 and 1), typically for binary or multi-class outcomes. It uses a sigmoid function to model the relationship.\n",
        "\n",
        "**Linear Regression** is used for regression tasks, predicting continuous values (e.g., prices or measurements). It models a linear relationship between the input variables and the output.\n",
        "\n",
        "### Key Differences:\n",
        "- **Logistic Regression**: Classification, outputs probabilities, uses the sigmoid function.\n",
        "- **Linear Regression**: Regression, outputs continuous values, no activation function.\n",
        "\n",
        "##Q2)What is the mathematical equation of Logistic Regression ?\n",
        "\n",
        "Ans:The mathematical equation for **Logistic Regression** is:\n",
        "\n",
        "\\[\n",
        "P(y = 1 | X) = \\frac{1}{1 + e^{-(b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n)}}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( P(y = 1 | X) \\) is the probability that the target variable \\( y \\) is 1 given the input features \\( X \\).\n",
        "- \\( b_0 \\) is the intercept (bias term).\n",
        "- \\( b_1, b_2, \\dots, b_n \\) are the coefficients of the input features \\( X_1, X_2, \\dots, X_n \\).\n",
        "- \\( e \\) is the base of the natural logarithm (approx. 2.718).\n",
        "\n",
        "The expression inside the sigmoid function, \\( (b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n) \\), is a linear combination of the input features, and the sigmoid function \\( \\frac{1}{1 + e^{-z}} \\) squashes the output to a value between 0 and 1, representing the probability.\n",
        "\n",
        "##Q3)Why do we use the Sigmoid function in Logistic Regression ?\n",
        "\n",
        "Ans:We use the **sigmoid function** in **Logistic Regression** for the following reasons:\n",
        "\n",
        "1. **Probability Output**: The sigmoid function transforms any real-valued number (the linear combination of features) into a value between 0 and 1. This is crucial because logistic regression is used for classification problems, and we need to interpret the output as a probability (e.g., the probability of the target being class 1).\n",
        "\n",
        "   \\[\n",
        "   \\text{Sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
        "   \\]\n",
        "   where \\( z = b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n \\).\n",
        "\n",
        "2. **Non-linearity**: The sigmoid function introduces non-linearity, which allows logistic regression to model complex relationships between the input variables and the target variable. While the input-output relationship is linear before applying the sigmoid, the sigmoid ensures the final output is a non-linear mapping.\n",
        "\n",
        "3. **Interpretability**: The sigmoid function gives outputs in the form of probabilities, which are easy to interpret. For example, if the output is 0.8, it means there is an 80% chance of the target being class 1 (or true, depending on the context).\n",
        "\n",
        "4. **Bounding**: The sigmoid function ensures that the predicted values are always between 0 and 1, which helps in classification tasks where a decision threshold (e.g., 0.5) can be applied to classify the output as either class 0 or class 1.\n",
        "\n",
        "##Q4)What is the cost function of Logistic Regression ?\n",
        "\n",
        "Ans:The cost function of **Logistic Regression** is called the **Log-Loss** or **Binary Cross-Entropy**. It measures the difference between the predicted probabilities and the actual class labels. The goal is to minimize this cost function during training to improve the model's accuracy.\n",
        "\n",
        "For **binary classification**, the cost function is given by:\n",
        "\n",
        "\\[\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( J(\\theta) \\) is the cost function.\n",
        "- \\( m \\) is the number of training examples.\n",
        "- \\( y^{(i)} \\) is the true label for the \\( i \\)-th training example (0 or 1).\n",
        "- \\( h_\\theta(x^{(i)}) \\) is the predicted probability (the output of the sigmoid function), given by:\n",
        "  \n",
        "  \\[\n",
        "  h_\\theta(x^{(i)}) = \\frac{1}{1 + e^{-\\theta^T x^{(i)}}}\n",
        "  \\]\n",
        "  where \\( \\theta \\) is the vector of parameters (weights), and \\( x^{(i)} \\) is the feature vector of the \\( i \\)-th training example.\n",
        "\n",
        "### Key Points:\n",
        "- The cost function penalizes incorrect predictions more heavily when they are confident (i.e., when the model is very sure but wrong).\n",
        "- The goal is to minimize this cost function during training to find the optimal parameters \\( \\theta \\).\n",
        "  \n",
        "This is the cost function for **binary classification**. For **multi-class classification**, the cost function is extended to **Categorical Cross-Entropy** (Softmax loss).\n",
        "\n",
        "##Q5)What is Regularization in Logistic Regression? Why is it needed ?\n",
        "\n",
        "Ans:### **Regularization in Logistic Regression**:\n",
        "**Regularization** is a technique used to prevent **overfitting** in logistic regression (and other machine learning models) by adding a penalty term to the cost function. This penalty discourages the model from learning overly complex patterns, which might not generalize well to new, unseen data.\n",
        "\n",
        "### Why Regularization is Needed:\n",
        "1. **Prevents Overfitting**: When a model learns too many details or noise from the training data, it performs very well on the training set but poorly on new data (testing set). Regularization helps to make the model simpler and generalize better.\n",
        "   \n",
        "2. **Controls Model Complexity**: Without regularization, the model might assign excessively large weights to some features, which could lead to an overly complex model that fits the training data too perfectly. Regularization discourages such large coefficients.\n",
        "\n",
        "3. **Improves Generalization**: By adding a penalty to the cost function, regularization helps the model focus on the most important features, leading to better performance on unseen data.\n",
        "\n",
        "### Types of Regularization in Logistic Regression:\n",
        "1. **L2 Regularization (Ridge Regularization)**:\n",
        "   - Adds the **sum of squared coefficients** (except the intercept) to the cost function.\n",
        "   - The cost function becomes:\n",
        "\n",
        "   \\[\n",
        "   J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2\n",
        "   \\]\n",
        "\n",
        "   - Where \\( \\lambda \\) is the **regularization parameter** (also called the penalty term). The larger the \\( \\lambda \\), the stronger the regularization.\n",
        "\n",
        "2. **L1 Regularization (Lasso Regularization)**:\n",
        "   - Adds the **sum of absolute values of the coefficients** to the cost function.\n",
        "   - The cost function becomes:\n",
        "\n",
        "   \\[\n",
        "   J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "   \\]\n",
        "\n",
        "   - L1 regularization can lead to sparse solutions (some coefficients being exactly zero), making it useful for feature selection.\n",
        "\n",
        "### How Regularization Works:\n",
        "- **L2 Regularization**: The penalty term \\( \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2 \\) penalizes large weights, forcing the model to keep weights small but not necessarily zero. This makes the model less sensitive to fluctuations in the training data and helps it generalize better.\n",
        "  \n",
        "- **L1 Regularization**: The penalty term \\( \\lambda \\sum_{j=1}^{n} |\\theta_j| \\) can force some weights to become exactly zero, effectively performing **feature selection**. This is useful when you want a sparse model with only the most relevant features.\n",
        "\n",
        "### When to Use Regularization:\n",
        "- **Overfitting**: If your model is overfitting (i.e., performing well on the training data but poorly on validation/testing data), adding regularization can help.\n",
        "- **High Dimensional Data**: When the number of features is large (e.g., many irrelevant features), regularization helps to reduce the influence of less important features.\n",
        "\n",
        "##Q6)Explain the difference between Lasso, Ridge, and Elastic Net regression ?\n",
        "\n",
        "Ans:**Lasso**, **Ridge**, and **Elastic Net** are all types of **regularization** techniques used in regression models to prevent overfitting and improve generalization. Each method applies a different form of penalty to the coefficients of the model. Here's a breakdown of the differences:\n",
        "\n",
        "### 1. **Ridge Regression (L2 Regularization)**:\n",
        "- **Penalty**: Adds the **sum of squared coefficients** (L2 norm) to the cost function.\n",
        "  \n",
        "  The cost function becomes:\n",
        "  \\[\n",
        "  J(\\theta) = \\text{RSS} + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "  \\]\n",
        "  where:\n",
        "  - \\( \\text{RSS} \\) is the residual sum of squares (the regular regression loss).\n",
        "  - \\( \\lambda \\) is the regularization parameter that controls the strength of the penalty.\n",
        "  - \\( \\theta_j \\) are the model parameters.\n",
        "\n",
        "- **Effect**: Ridge regression **shrinks the coefficients** towards zero but **does not set any of them exactly to zero**. It helps prevent large weights that could lead to overfitting, but it does not perform feature selection.\n",
        "\n",
        "- **When to Use**: Ridge is useful when you have many small or moderately correlated features and want to shrink the coefficients evenly.\n",
        "\n",
        "### 2. **Lasso Regression (L1 Regularization)**:\n",
        "- **Penalty**: Adds the **sum of absolute values of the coefficients** (L1 norm) to the cost function.\n",
        "  \n",
        "  The cost function becomes:\n",
        "  \\[\n",
        "  J(\\theta) = \\text{RSS} + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "  \\]\n",
        "  \n",
        "- **Effect**: Lasso regression **shrinks some coefficients exactly to zero**, effectively performing **feature selection**. This means that Lasso can help eliminate irrelevant or less important features entirely from the model.\n",
        "\n",
        "- **When to Use**: Lasso is useful when you expect that only a small subset of features are important for prediction, and you want to automatically exclude irrelevant ones.\n",
        "\n",
        "### 3. **Elastic Net Regression**:\n",
        "- **Penalty**: Combines both **L1 (Lasso)** and **L2 (Ridge)** penalties.\n",
        "  \n",
        "  The cost function becomes:\n",
        "  \\[\n",
        "  J(\\theta) = \\text{RSS} + \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\frac{\\lambda_2}{2} \\sum_{j=1}^{n} \\theta_j^2\n",
        "  \\]\n",
        "  where:\n",
        "  - \\( \\lambda_1 \\) and \\( \\lambda_2 \\) are the regularization parameters for L1 and L2 penalties, respectively.\n",
        "\n",
        "- **Effect**: Elastic Net **penalizes large coefficients** like Ridge (L2), but it can also set some coefficients to zero like Lasso (L1). This allows it to perform **feature selection** while also handling correlated features more effectively.\n",
        "\n",
        "- **When to Use**: Elastic Net is useful when you have many features, some of which are highly correlated, and you want the benefits of both Lasso and Ridge. It is often used when the number of features is large or the data contains multicollinearity.\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "| **Method**             | **Penalty**                          | **Effect on Coefficients**                          | **When to Use**                                                                 |\n",
        "|-----------------------|--------------------------------------|----------------------------------------------------|--------------------------------------------------------------------------------|\n",
        "| **Ridge**             | L2 (sum of squared coefficients)     | Shrinks coefficients towards zero (but does not eliminate them) | When features are numerous and there is no need for feature selection.         |\n",
        "| **Lasso**             | L1 (sum of absolute coefficients)    | Shrinks some coefficients to exactly zero (feature selection) | When expecting many irrelevant features and want to perform automatic feature selection. |\n",
        "| **Elastic Net**       | Combination of L1 and L2 penalties   | Shrinks coefficients and performs feature selection, handling correlated features | When dealing with many features, some of which are highly correlated.         |\n",
        "\n",
        "##Q7)When should we use Elastic Net instead of Lasso or Ridge ?\n",
        "\n",
        "Ans:You should use **Elastic Net** instead of **Lasso** or **Ridge** in the following situations:\n",
        "\n",
        "### 1. **When Features Are Highly Correlated**:\n",
        "   - If you have many features that are **highly correlated** (multicollinearity), **Elastic Net** is a better choice than **Lasso**.\n",
        "   - **Lasso** may randomly pick one feature from a correlated group and discard the others, which can lead to unstable models. **Elastic Net**, by combining both **L1 (Lasso)** and **L2 (Ridge)** regularization, can handle correlated features more effectively by distributing the penalty across correlated features, rather than selecting one.\n",
        "\n",
        "### 2. **When You Have Many Features (Some of Which Are Irrelevant)**:\n",
        "   - If the number of features is very high and many of them are **irrelevant**, **Lasso** would be good since it can shrink some coefficients to zero, performing **automatic feature selection**. However, if the features are highly correlated, **Elastic Net** is often preferred because it strikes a balance between **feature selection** (like Lasso) and **handling correlations** (like Ridge).\n",
        "\n",
        "### 3. **When You Need a Balance Between Lasso and Ridge**:\n",
        "   - **Elastic Net** allows you to control the mix between **L1** and **L2** regularization using the **alpha parameter**:\n",
        "     - When \\( \\alpha = 1 \\), it behaves like **Lasso** (L1 regularization only).\n",
        "     - When \\( \\alpha = 0 \\), it behaves like **Ridge** (L2 regularization only).\n",
        "     - When \\( 0 < \\alpha < 1 \\), it combines the effects of **Lasso** and **Ridge**.\n",
        "   - This flexibility is useful when you want to tune the model to strike the right balance between **feature selection** and **shrinkage**.\n",
        "\n",
        "### 4. **When You're Unsure Which to Choose (Lasso vs Ridge)**:\n",
        "   - If you're unsure whether to use **Lasso** or **Ridge**, **Elastic Net** offers the best of both worlds. It can perform **feature selection** (like Lasso) while also handling **multicollinearity** (like Ridge). You can adjust the **alpha** parameter to determine the balance that works best for your dataset.\n",
        "\n",
        "### 5. **When You Have a Large Number of Features and Small Sample Size**:\n",
        "   - In cases where the number of features (\\( p \\)) is large compared to the number of data points (\\( m \\)), **Elastic Net** can help by reducing model complexity through both regularization techniques, especially when there are correlations between features.\n",
        "\n",
        "##Q8)What is the impact of the regularization parameter (λ) in Logistic Regression ?\n",
        "\n",
        "Ans:The regularization parameter \\( \\lambda \\) (lambda) in **Logistic Regression** plays a crucial role in controlling the strength of regularization. Regularization helps to prevent overfitting by penalizing large coefficients in the model. Here's how \\( \\lambda \\) impacts the logistic regression model:\n",
        "\n",
        "### **Impact of \\( \\lambda \\) on the Model**:\n",
        "\n",
        "1. **When \\( \\lambda \\) is Small (Near 0)**:\n",
        "   - **Less Regularization**: When \\( \\lambda \\) is very small, regularization has little effect on the model. The model will focus more on fitting the training data closely, potentially leading to **overfitting**.\n",
        "   - **Model Complexity**: The coefficients can grow very large, and the model may start to memorize the training data rather than generalize to new, unseen data.\n",
        "\n",
        "2. **When \\( \\lambda \\) is Large**:\n",
        "   - **More Regularization**: A larger \\( \\lambda \\) imposes a stronger penalty on the coefficients, which forces the model to reduce the magnitude of its coefficients. This leads to **simpler models** with smaller weights and less complexity.\n",
        "   - **Underfitting**: If \\( \\lambda \\) is too large, the model may become **too simple** and fail to capture important patterns in the data, leading to **underfitting**.\n",
        "   - The model may shrink coefficients to zero, especially in Lasso (L1 regularization), which results in **feature selection**.\n",
        "\n",
        "3. **Balance Between Overfitting and Underfitting**:\n",
        "   - The key is to find the **optimal value of \\( \\lambda \\)**. When \\( \\lambda \\) is too small, the model risks **overfitting**, while when it's too large, it risks **underfitting**.\n",
        "   - Typically, this is done using techniques like **cross-validation**, where different values of \\( \\lambda \\) are tested, and the one that minimizes error on the validation data is selected.\n",
        "\n",
        "### **Visualizing the Effect of \\( \\lambda \\)**:\n",
        "\n",
        "- **Small \\( \\lambda \\)**: The model will closely fit the training data, with large coefficients (overfitting).\n",
        "- **Large \\( \\lambda \\)**: The model will have smaller coefficients, making the model simpler and potentially less flexible (underfitting).\n",
        "\n",
        "### **Impact on Different Types of Regularization**:\n",
        "\n",
        "- **L2 Regularization (Ridge)**: \\( \\lambda \\) controls the strength of the penalty on the sum of squared coefficients. Larger \\( \\lambda \\) values will result in smaller coefficients but will never push them exactly to zero.\n",
        "  \n",
        "- **L1 Regularization (Lasso)**: \\( \\lambda \\) controls the penalty on the sum of absolute values of the coefficients. Larger \\( \\lambda \\) values can shrink some coefficients to exactly zero, performing **feature selection**.\n",
        "  \n",
        "- **Elastic Net (Combination of L1 and L2)**: Here, \\( \\lambda \\) controls both L1 and L2 penalties, and its impact is a combination of both Lasso and Ridge effects. Larger \\( \\lambda \\) values will shrink coefficients more and may lead to some being set to zero (feature selection).\n",
        "\n",
        "##Q9)What are the key assumptions of Logistic Regression ?\n",
        "\n",
        "Ans:**Logistic Regression** is a popular algorithm for classification tasks. While it is quite flexible and effective, there are several **key assumptions** underlying the model that should be considered for optimal performance:\n",
        "\n",
        "### 1. **Linearity of the Log-Odds**:\n",
        "   - **Assumption**: Logistic regression assumes that the relationship between the **independent variables** (predictors) and the **log-odds of the dependent variable** is **linear**.\n",
        "   - **Explanation**: This means that for binary classification, the log of the odds (logit) of the dependent variable is a linear combination of the predictor variables.\n",
        "     \\[\n",
        "     \\log\\left(\\frac{P(y=1|X)}{P(y=0|X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n\n",
        "     \\]\n",
        "   - **Impact**: If this linearity assumption is violated (e.g., the relationship is nonlinear), the model might not perform well. Nonlinear relationships may need to be transformed or addressed with other models (like decision trees).\n",
        "\n",
        "### 2. **Independence of Observations**:\n",
        "   - **Assumption**: Logistic regression assumes that the observations (data points) are **independent** of each other.\n",
        "   - **Explanation**: There should be no correlation between the residuals (errors) or between the observations. Each data point should provide independent information.\n",
        "   - **Impact**: If there is dependence (e.g., in time series data or clustered data), the model’s estimates might be biased or incorrect. In such cases, techniques like **generalized estimating equations (GEE)** or **mixed-effects models** may be more appropriate.\n",
        "\n",
        "### 3. **No or Little Multicollinearity**:\n",
        "   - **Assumption**: The predictors (independent variables) should not be **highly correlated** with each other. High multicollinearity makes it difficult to estimate the individual effect of each predictor.\n",
        "   - **Explanation**: When two or more predictors are highly correlated, it can lead to unstable estimates of the regression coefficients. This can inflate the variance of the coefficients and make the model sensitive to small changes in the data.\n",
        "   - **Impact**: Multicollinearity can lead to overfitting and unreliable coefficient estimates. It can be detected using tools like **Variance Inflation Factor (VIF)**.\n",
        "\n",
        "### 4. **Large Sample Size**:\n",
        "   - **Assumption**: Logistic regression generally requires a **sufficiently large sample size** for accurate and reliable estimation of the coefficients, particularly when there are many predictors.\n",
        "   - **Explanation**: With a small sample size, the maximum likelihood estimation may lead to biased or unstable estimates. Having a large enough sample ensures that the model can effectively learn the relationships between the predictors and the outcome.\n",
        "   - **Impact**: If the sample size is too small, the model may have high variance and not generalize well to new data.\n",
        "\n",
        "### 5. **Binary or Multinomial Dependent Variable**:\n",
        "   - **Assumption**: Logistic regression is typically used for **binary** outcomes, where the dependent variable has two possible outcomes (e.g., 0 or 1, true or false). For multiclass classification problems, a variant like **Multinomial Logistic Regression** can be used.\n",
        "   - **Explanation**: The dependent variable must be categorical (binary or multinomial). Logistic regression is not suitable for continuous outcomes.\n",
        "   - **Impact**: If the dependent variable is continuous, logistic regression is not appropriate, and other regression methods (e.g., linear regression) should be used.\n",
        "\n",
        "### 6. **No Extreme Outliers**:\n",
        "   - **Assumption**: Logistic regression assumes that there are no **extreme outliers** in the predictors.\n",
        "   - **Explanation**: Outliers can distort the relationship between the predictors and the outcome, affecting the estimates of the model’s parameters.\n",
        "   - **Impact**: Outliers may have a disproportionate effect on the model, making the coefficient estimates unreliable. It’s important to identify and deal with outliers, either by removing them or using robust regression methods.\n",
        "\n",
        "### 7. **Homoscedasticity (Constant Variance of Errors)**:\n",
        "   - **Assumption**: While logistic regression does not require homoscedasticity in the same way as linear regression (because the outcome is categorical), it’s still assumed that the variance of the errors remains roughly constant across the values of the predictors.\n",
        "   - **Explanation**: Homoscedasticity means that the spread of the residuals or errors should not change systematically with the value of the independent variables.\n",
        "   - **Impact**: If there is heteroscedasticity (non-constant variance of errors), it could indicate model misspecification, though this is less of a concern for logistic regression than for linear regression.\n",
        "\n",
        "### 8. **Adequate Representation of Each Class (Balanced Classes)**:\n",
        "   - **Assumption**: Logistic regression assumes that both classes (in binary classification) or each category (in multinomial classification) have an **adequate number of observations**.\n",
        "   - **Explanation**: If one class is vastly underrepresented (imbalanced classes), the model may be biased toward predicting the majority class. This can result in poor performance in predicting the minority class.\n",
        "   - **Impact**: Imbalanced datasets may lead to inaccurate predictions and require techniques such as **resampling**, **class weighting**, or **synthetic data generation (SMOTE)**.\n",
        "\n",
        "---\n",
        "\n",
        "##Q10)What are some alternatives to Logistic Regression for classification tasks ?\n",
        "\n",
        "Ans:Here are some common alternatives to **Logistic Regression** for classification tasks:\n",
        "\n",
        "1. **Decision Trees**: Simple and interpretable, but prone to overfitting.\n",
        "2. **Random Forests**: Ensemble of decision trees, robust to overfitting, but less interpretable.\n",
        "3. **Support Vector Machines (SVM)**: Effective for high-dimensional data and complex decision boundaries.\n",
        "4. **k-Nearest Neighbors (k-NN)**: Simple, instance-based learning, but computationally expensive.\n",
        "5. **Naive Bayes**: Fast and works well with text data, but assumes feature independence.\n",
        "6. **Gradient Boosting Machines (GBM)**: High accuracy with ensemble learning (e.g., XGBoost, LightGBM), but computationally expensive.\n",
        "7. **Neural Networks**: Can model complex relationships, but require large data and computational resources.\n",
        "8. **Linear Discriminant Analysis (LDA)**: Assumes normality, works well for linear separability.\n",
        "9. **Quadratic Discriminant Analysis (QDA)**: Similar to LDA but with more flexibility for decision boundaries.\n",
        "\n",
        "Each alternative has its strengths and is chosen based on the dataset size, complexity, and required interpretability.\n",
        "\n",
        "##Q11)What are Classification Evaluation Metrics ?\n",
        "\n",
        "Ans:Classification evaluation metrics are used to assess the performance of classification models. Key metrics include:\n",
        "\n",
        "1. **Accuracy**: Proportion of correct predictions.\n",
        "2. **Precision**: Proportion of positive predictions that are correct.\n",
        "3. **Recall**: Proportion of actual positives correctly identified.\n",
        "4. **F1-Score**: Harmonic mean of precision and recall.\n",
        "5. **Specificity**: Proportion of actual negatives correctly identified.\n",
        "6. **ROC Curve**: Plots true positive rate vs. false positive rate at various thresholds.\n",
        "7. **AUC (Area Under the Curve)**: Summary of model's ability to distinguish between classes.\n",
        "8. **Confusion Matrix**: Summary of true positives, false positives, true negatives, and false negatives.\n",
        "9. **Log Loss**: Measures the performance based on predicted probabilities.\n",
        "10. **Matthews Correlation Coefficient (MCC)**: Measures binary classification quality.\n",
        "11. **Hamming Loss**: Fraction of incorrectly predicted labels in multi-label classification.\n",
        "\n",
        "These metrics help evaluate the model's effectiveness and decision trade-offs.\n",
        "\n",
        "##Q12)How does class imbalance affect Logistic Regression ?\n",
        "\n",
        "Ans:Class imbalance can significantly affect the performance of **Logistic Regression** and other machine learning models. In a scenario where one class is much more frequent than the other (e.g., 90% of data belongs to class A and 10% to class B), Logistic Regression may struggle to correctly predict the minority class, leading to several issues:\n",
        "\n",
        "### 1. **Bias Toward the Majority Class**:\n",
        "   - Logistic Regression tends to predict the majority class more often, as it minimizes the overall error (or cost) during training. As a result, it may predict the majority class most of the time and ignore the minority class, leading to **low recall** for the minority class.\n",
        "\n",
        "### 2. **Skewed Decision Boundary**:\n",
        "   - In imbalanced datasets, the decision boundary between classes may be skewed toward the majority class. This happens because the model learns to prioritize minimizing errors for the majority class, potentially increasing false negatives for the minority class.\n",
        "\n",
        "### 3. **Metrics Misleading**:\n",
        "   - **Accuracy** becomes less useful in imbalanced settings, as a high accuracy could be achieved by simply predicting the majority class all the time. The model might perform poorly on the minority class but still appear accurate.\n",
        "\n",
        "### 4. **Overfitting or Underfitting**:\n",
        "   - Logistic Regression might overfit the minority class if the model becomes too complex trying to fit the small number of positive examples. On the other hand, it may underfit the minority class by failing to identify relevant patterns due to the data imbalance.\n",
        "\n",
        "### 5. **Evaluation Metric Imbalance**:\n",
        "   - Precision, recall, F1-score, and AUC-ROC are more reliable than accuracy for imbalanced classes. Without these metrics, the performance of the minority class can go unnoticed.\n",
        "\n",
        "##Q13)What is Hyperparameter Tuning in Logistic Regression ?\n",
        "\n",
        "Ans:**Hyperparameter tuning** in Logistic Regression is the process of selecting the best hyperparameters (pre-set values) to optimize model performance. Key hyperparameters include:\n",
        "\n",
        "1. **Regularization strength (C)**: Controls overfitting by adjusting penalty on large coefficients.\n",
        "2. **Regularization type (penalty)**: 'l1' for Lasso or 'l2' for Ridge regularization.\n",
        "3. **Solver**: The optimization algorithm used (e.g., 'liblinear', 'lbfgs').\n",
        "4. **Max iterations (max_iter)**: Maximum iterations for convergence.\n",
        "5. **Class weight**: Balances classes in case of imbalanced data.\n",
        "\n",
        "Common tuning methods include **Grid Search**, **Random Search**, and **Bayesian Optimization** to find the best combination of hyperparameters that improve the model’s accuracy and generalization.\n",
        "\n",
        "\n",
        "##Q14)What are different solvers in Logistic Regression? Which one should be used ?\n",
        "\n",
        "Ans:In **Logistic Regression**, the **solver** is the optimization algorithm used to find the best parameters (weights) that minimize the cost function. Different solvers have different strengths depending on the size of the dataset, the type of regularization used, and the number of features. Here are the main solvers available in **Logistic Regression** (e.g., in **Scikit-learn**):\n",
        "\n",
        "### 1. **'liblinear'**:\n",
        "   - **Description**: A solver based on the **coordinate descent** algorithm. It supports both **L1** (Lasso) and **L2** (Ridge) regularization.\n",
        "   - **Best For**: Small datasets, binary classification, and when using **L1** regularization (sparse solutions).\n",
        "   - **Limitations**: May not scale well for large datasets.\n",
        "   - **Use Case**: When the dataset is small or when you need sparsity in the solution (with L1 regularization).\n",
        "\n",
        "### 2. **'newton-cg'**:\n",
        "   - **Description**: A second-order optimization method that uses the **Newton's method** to converge to the solution.\n",
        "   - **Best For**: Larger datasets, when using **L2** regularization, and **multinomial** logistic regression (multi-class).\n",
        "   - **Limitations**: More computationally expensive than others in some cases.\n",
        "   - **Use Case**: When dealing with larger datasets and multinomial classification problems.\n",
        "\n",
        "### 3. **'lbfgs'**:\n",
        "   - **Description**: A variant of **Broyden–Fletcher–Goldfarb–Shanno** (BFGS) algorithm. It's an approximation of Newton's method using limited memory.\n",
        "   - **Best For**: Large datasets and **multinomial** classification with **L2** regularization.\n",
        "   - **Limitations**: Cannot handle **L1** regularization.\n",
        "   - **Use Case**: When working with larger datasets, especially for multinomial logistic regression and when using **L2** regularization.\n",
        "\n",
        "### 4. **'saga'**:\n",
        "   - **Description**: A variant of **Stochastic Gradient Descent (SGD)**, it handles large datasets and supports **L1** regularization (like 'liblinear') and **L2** regularization.\n",
        "   - **Best For**: Large datasets, **L1** and **L2** regularization, and **multinomial** logistic regression.\n",
        "   - **Limitations**: Slower than other solvers for small datasets, and can be computationally expensive with small data.\n",
        "   - **Use Case**: When dealing with large datasets and needing both **L1** and **L2** regularization or when using multinomial classification.\n",
        "\n",
        "### 5. **'saga'** vs **'liblinear'**:\n",
        "   - **'saga'** can handle **L1** regularization and is more efficient for **large datasets**, whereas **'liblinear'** is better for **small datasets** with binary classification and when sparsity is required (with **L1** regularization).\n",
        "\n",
        "---\n",
        "\n",
        "##Q15)How is Logistic Regression extended for multiclass classification ?\n",
        "\n",
        "Ans:Logistic Regression can be extended to multiclass classification using two main methods:\n",
        "\n",
        "1. **One-vs-Rest (OvR)**:\n",
        "   - Train a separate binary classifier for each class, where each model predicts whether an instance belongs to that class or not. The class with the highest probability from these classifiers is the predicted class.\n",
        "\n",
        "2. **Multinomial Logistic Regression (Softmax)**:\n",
        "   - Use a single model that predicts the probabilities for all classes simultaneously using the Softmax function. The class with the highest probability is chosen as the predicted class.\n",
        "\n",
        "**In short**: OvR involves multiple binary classifiers, while multinomial logistic regression uses a single model with Softmax to directly output probabilities for all classes.\n",
        "\n",
        "##Q16)What are the advantages and disadvantages of Logistic Regression ?\n",
        "\n",
        "Ans:### Advantages of Logistic Regression:\n",
        "\n",
        "1. **Simplicity and Interpretability**:\n",
        "   - Logistic regression is easy to implement and understand. The output is straightforward, providing probabilities that can be interpreted as the likelihood of a given outcome.\n",
        "\n",
        "2. **Efficiency**:\n",
        "   - It is computationally inexpensive and runs quickly, making it suitable for smaller datasets and when model interpretability is important.\n",
        "\n",
        "3. **Probabilistic Output**:\n",
        "   - Logistic regression provides probabilistic predictions, which can be useful when you need to make decisions based on probabilities (e.g., risk assessment).\n",
        "\n",
        "4. **Works Well for Linearly Separable Data**:\n",
        "   - Performs well when the data is linearly separable or nearly linear, as the decision boundary is simple and easy to compute.\n",
        "\n",
        "5. **Regularization Support**:\n",
        "   - Logistic regression can easily incorporate **regularization** (L1 or L2) to avoid overfitting, especially when dealing with high-dimensional data.\n",
        "\n",
        "6. **Multiclass Extension**:\n",
        "   - It can be extended to handle multiclass problems using techniques like **One-vs-Rest (OvR)** or **Multinomial Logistic Regression (Softmax)**.\n",
        "\n",
        "---\n",
        "\n",
        "### Disadvantages of Logistic Regression:\n",
        "\n",
        "1. **Linear Decision Boundaries**:\n",
        "   - Logistic regression assumes that the relationship between the features and the target is linear. It struggles with complex, non-linear relationships unless you transform the features or use polynomial features.\n",
        "\n",
        "2. **Sensitive to Outliers**:\n",
        "   - Logistic regression can be sensitive to outliers, as they can disproportionately influence the model’s decision boundary.\n",
        "\n",
        "3. **Performance on Complex Data**:\n",
        "   - It may not perform well on complex data patterns where nonlinear classifiers (like decision trees or neural networks) can capture more intricate relationships.\n",
        "\n",
        "4. **Feature Engineering**:\n",
        "   - Logistic regression requires careful feature selection or transformation (e.g., polynomial features) to improve performance on non-linear problems, making it less flexible than some other models.\n",
        "\n",
        "5. **Assumption of Independence**:\n",
        "   - It assumes that features are independent of each other, which might not always hold true, especially in cases with correlated features (though regularization can help mitigate this).\n",
        "\n",
        "6. **Inability to Handle Large Data with Many Features Well**:\n",
        "   - While regularization can help, logistic regression may struggle with high-dimensional datasets unless additional techniques like feature selection or dimensionality reduction are applied.\n",
        "\n",
        "---\n",
        "\n",
        "##Q17)What are some use cases of Logistic Regression ?\n",
        "\n",
        "Ans:Logistic Regression is a versatile and widely used algorithm in machine learning, particularly for **binary classification** and **multiclass classification** problems. Here are some common use cases:\n",
        "\n",
        "### 1. **Spam Detection**:\n",
        "   - **Problem**: Classifying emails as \"spam\" or \"not spam.\"\n",
        "   - **How it Works**: Logistic regression can be used to predict the probability of an email being spam based on features like the presence of certain words, the sender's address, and email metadata.\n",
        "\n",
        "### 2. **Customer Churn Prediction**:\n",
        "   - **Problem**: Predicting whether a customer will leave (churn) or stay with a service.\n",
        "   - **How it Works**: Logistic regression models customer behavior and demographic features to predict the likelihood of churn (e.g., in telecom, insurance, or subscription services).\n",
        "\n",
        "### 3. **Credit Scoring**:\n",
        "   - **Problem**: Assessing the likelihood of a borrower defaulting on a loan.\n",
        "   - **How it Works**: Logistic regression can predict whether an individual is likely to default based on financial features, such as income, credit history, and loan amount.\n",
        "\n",
        "### 4. **Disease Prediction (Medical Diagnosis)**:\n",
        "   - **Problem**: Predicting the presence or absence of a disease (e.g., diabetes, cancer).\n",
        "   - **How it Works**: Logistic regression is used to classify whether a patient has a certain disease based on medical features like age, weight, test results, and other health metrics.\n",
        "\n",
        "### 5. **Marketing Campaign Effectiveness**:\n",
        "   - **Problem**: Predicting whether a customer will respond to a marketing campaign.\n",
        "   - **How it Works**: Logistic regression can be used to model the likelihood of a customer engaging with a marketing offer, given features like past purchase behavior, demographics, and campaign attributes.\n",
        "\n",
        "### 6. **Credit Card Fraud Detection**:\n",
        "   - **Problem**: Detecting fraudulent transactions.\n",
        "   - **How it Works**: Logistic regression models the probability that a transaction is fraudulent based on transaction features such as amount, location, and transaction history.\n",
        "\n",
        "### 7. **Political Polling and Election Prediction**:\n",
        "   - **Problem**: Predicting the outcome of an election or political vote.\n",
        "   - **How it Works**: Logistic regression can predict voter behavior (e.g., likelihood of voting for a particular candidate) based on factors like demographics, past voting patterns, and survey data.\n",
        "\n",
        "### 8. **Sentiment Analysis**:\n",
        "   - **Problem**: Classifying text or social media posts as positive or negative (e.g., product reviews).\n",
        "   - **How it Works**: Logistic regression is often used to classify text data into binary categories (positive or negative sentiment) based on features like the presence of certain words, phrases, or emoticons.\n",
        "\n",
        "### 9. **Web Traffic Prediction**:\n",
        "   - **Problem**: Predicting whether a website visitor will convert (e.g., make a purchase or sign up).\n",
        "   - **How it Works**: Logistic regression can be used to predict the likelihood that a user will convert based on browsing behavior, referral source, and user demographics.\n",
        "\n",
        "### 10. **Multiclass Classification**:\n",
        "   - **Problem**: Classifying data into more than two categories, such as handwriting recognition or categorizing types of plants.\n",
        "   - **How it Works**: By using techniques like **One-vs-Rest (OvR)** or **Multinomial Logistic Regression (Softmax)**, logistic regression can be extended to handle problems with more than two categories.\n",
        "\n",
        "### 11. **Image Classification** (with feature extraction):\n",
        "   - **Problem**: Classifying images based on their features.\n",
        "   - **How it Works**: Logistic regression can classify images (e.g., distinguishing between types of objects in images) when combined with feature extraction techniques (like SIFT, HOG, or CNN-based features).\n",
        "\n",
        "---\n",
        "\n",
        "##Q18)What is the difference between Softmax Regression and Logistic Regression ?\n",
        "\n",
        "Ans:The key difference between **Softmax Regression** and **Logistic Regression** is:\n",
        "\n",
        "- **Logistic Regression**: Used for **binary classification** (two classes), where it predicts the probability of an instance belonging to one of two classes using the **sigmoid function**.\n",
        "\n",
        "- **Softmax Regression**: Used for **multiclass classification** (three or more classes), where it predicts probabilities for each class using the **Softmax function**, and the class with the highest probability is chosen.\n",
        "\n",
        "**In short**: Logistic regression is for binary classification, while Softmax regression is an extension of logistic regression for multiclass classification.\n",
        "\n",
        "##Q19)How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification ?\n",
        "\n",
        "Ans:Choose **One-vs-Rest (OvR)** when:\n",
        "\n",
        "- You have **imbalanced classes** or independent classes.\n",
        "- You want **multiple binary classifiers** (which may offer more flexibility).\n",
        "- The model interpretability for each class is important.\n",
        "\n",
        "Choose **Softmax** when:\n",
        "\n",
        "- You have **mutually exclusive classes**.\n",
        "- You prefer a **single model** that directly handles multiclass classification.\n",
        "- Your classes are **well-separated** and efficiency is a priority.\n",
        "\n",
        "In general, **Softmax** is better for multiclass problems with exclusive classes, while **OvR** is useful when classes are independent or imbalanced.\n",
        "\n",
        "##Q20)How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Ans:In **Logistic Regression**, the coefficients (weights) represent the relationship between the input features and the log-odds of the target variable. Here's how to interpret them:\n",
        "\n",
        "### 1. **Understanding the Logistic Regression Model**:\n",
        "   The logistic regression model predicts the probability of an event occurring (e.g., class 1) as:\n",
        "   \\[\n",
        "   P(y = 1 | X) = \\frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n)}}\n",
        "   \\]\n",
        "   where:\n",
        "   - \\( w_0 \\) is the intercept.\n",
        "   - \\( w_1, w_2, ..., w_n \\) are the coefficients for the features \\( x_1, x_2, ..., x_n \\).\n",
        "\n",
        "   The coefficients (\\(w_i\\)) influence the log-odds of the outcome:\n",
        "   \\[\n",
        "   \\text{Log-Odds} = \\log \\left( \\frac{P(y = 1)}{P(y = 0)} \\right) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
        "   \\]\n",
        "\n",
        "### 2. **Interpreting the Coefficients**:\n",
        "   - Each **coefficient** (\\( w_i \\)) measures the effect of the corresponding feature (\\( x_i \\)) on the log-odds of the outcome.\n",
        "   - If \\( w_i \\) is **positive**, an increase in the feature \\( x_i \\) increases the log-odds of the outcome, meaning it makes the outcome (e.g., class 1) more likely.\n",
        "   - If \\( w_i \\) is **negative**, an increase in the feature \\( x_i \\) decreases the log-odds of the outcome, meaning it makes the outcome (e.g., class 1) less likely.\n",
        "\n",
        "### 3. **Interpretation in Terms of Odds**:\n",
        "   - The **exponentiation** of a coefficient \\( e^{w_i} \\) gives you the **odds ratio** for that feature:\n",
        "   \\[\n",
        "   \\text{Odds Ratio} = e^{w_i}\n",
        "   \\]\n",
        "   - If \\( w_i > 0 \\), the **odds ratio** \\( e^{w_i} > 1 \\), indicating that as \\( x_i \\) increases, the odds of the outcome occurring increase.\n",
        "   - If \\( w_i < 0 \\), the **odds ratio** \\( e^{w_i} < 1 \\), indicating that as \\( x_i \\) increases, the odds of the outcome occurring decrease.\n",
        "   - If \\( w_i = 0 \\), the **odds ratio** is 1, meaning the feature \\( x_i \\) has no effect on the odds of the outcome.\n",
        "\n",
        "### 4. **Example Interpretation**:\n",
        "   Suppose the logistic regression model is:\n",
        "   \\[\n",
        "   \\text{Log-Odds} = -3 + 2 \\cdot x_1 - 0.5 \\cdot x_2\n",
        "   \\]\n",
        "   - The **coefficient for \\( x_1 \\)** is 2, which means for each one-unit increase in \\( x_1 \\), the log-odds of the outcome being class 1 increases by 2. This translates to an **odds ratio of \\( e^2 \\approx 7.39 \\)**, meaning the odds of class 1 occurring increase by a factor of about 7.39 for each unit increase in \\( x_1 \\).\n",
        "   - The **coefficient for \\( x_2 \\)** is -0.5, which means for each one-unit increase in \\( x_2 \\), the log-odds of the outcome being class 1 decreases by 0.5. This translates to an **odds ratio of \\( e^{-0.5} \\approx 0.61 \\)**, meaning the odds of class 1 occurring decrease by a factor of 0.61 for each unit increase in \\( x_2 \\).\n",
        "\n",
        "##PRATICAL QUESTION##\n",
        "\n",
        "##Q1)Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "o1NjuW_FEepr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00LsOjJUTjft",
        "outputId": "5c8260cf-e2cc-43dd-d0bc-bfc21bb2ede5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2)C Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "_cbMq_s9TslX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization (Lasso): {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88_BWvKRUXjI",
        "outputId": "681d1ec5-0dad-491c-c5d4-dd5f73459b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization (Lasso): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3)Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "4-034ovBUg7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization (Ridge): {accuracy:.4f}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"\\nModel Coefficients (Weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExEbvff4U-YQ",
        "outputId": "16e45ed2-9682-4351-cb12-be78789d0bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization (Ridge): 1.0000\n",
            "\n",
            "Model Coefficients (Weights):\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4)Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')C\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "_UlVFqntVA72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with Elastic Net Regularization\n",
        "# Use l1_ratio to control the mix of L1 and L2 regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.4f}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"\\nModel Coefficients (Weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEODwBkWVQgI",
        "outputId": "d41402a7-e64f-41d2-d158-c49ca8051757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 1.0000\n",
            "\n",
            "Model Coefficients (Weights):\n",
            "[[ 0.38914547  1.76847074 -2.42368235 -0.70655987]\n",
            " [ 0.07686519  0.          0.         -0.58177977]\n",
            " [-1.25849407 -1.5288128   2.59534819  2.08035798]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5)Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'C"
      ],
      "metadata": {
        "id": "JqBxeXRJVb9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with One-vs-Rest (OvR) strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-Rest (OvR) Strategy: {accuracy:.4f}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"\\nModel Coefficients (Weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O588c5tXCCE",
        "outputId": "d6495bb4-105f-48ed-b763-fc1fa7f9d5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-Rest (OvR) Strategy: 1.0000\n",
            "\n",
            "Model Coefficients (Weights):\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q6)Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "uaseP2Z0XEgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the hyperparameters grid (C and penalty)\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2']  # Regularization types (L1 and L2)\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Perform the grid search on the training set\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nBest Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjHXsC7HXbPf",
        "outputId": "611b2a26-52e2-4bbf-b83d-28e93912d099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'C': 1, 'penalty': 'l2'}\n",
            "\n",
            "Best Model Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "20 fits failed out of a total of 40.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.93333333        nan 0.96666667        nan 0.94166667\n",
            "        nan 0.95      ]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q7) Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "CML4bpLWXxNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Initialize Stratified K-Fold Cross-Validation with 5 folds\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store accuracy for each fold\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate and print the average accuracy across all folds\n",
        "average_accuracy = np.mean(accuracies)\n",
        "print(f\"Average Accuracy using Stratified K-Fold Cross-Validation: {average_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfYFDmpbYOxg",
        "outputId": "22fcdcea-a317-4644-91c8-87437b631660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy using Stratified K-Fold Cross-Validation: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q8)Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy\n",
        "\n",
        "Ans:\n"
      ],
      "metadata": {
        "id": "vLH721-jYTAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from a CSV file\n",
        "# Replace 'your_dataset.csv' with the actual file path\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "print(data.head())\n",
        "\n",
        "# Assuming the target column is the last column, and the rest are features\n",
        "# Modify as needed to match the structure of your dataset\n",
        "X = data.iloc[:, :-1]  # Features (all columns except the last one)\n",
        "y = data.iloc[:, -1]   # Target (the last column)\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "bDzVq1_0ZWy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q10)Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "9kUS68C_ZZdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),        # Regularization strength (log scale)\n",
        "    'penalty': ['l1', 'l2'],            # Regularization types\n",
        "    'solver': ['liblinear', 'saga']     # Solvers that support L1 regularization and general optimization\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=100, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "# Perform the randomized search on the training set\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found by RandomizedSearchCV\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# Get the best model from RandomizedSearchCV\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nBest Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FVHjzGcaW0L",
        "outputId": "e8e52663-98b6-473e-bf07-59fc4f0ef46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 80 is smaller than n_iter=100. Running 80 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'solver': 'saga', 'penalty': 'l2', 'C': 0.23357214690901212}\n",
            "\n",
            "Best Model Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q10)Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "rO9rmOluaaV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Wrap the Logistic Regression model with OneVsOneClassifier\n",
        "ovo_model = OneVsOneClassifier(log_reg)\n",
        "\n",
        "# Train the model on the training set\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cObrAzYDa31v",
        "outputId": "212a6b94-1f77-4b35-9778-e93134c1c368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q11)Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "vxb0oPKva73o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# To make it a binary classification problem, we'll filter to only include two classes (e.g., class 0 and class 1)\n",
        "binary_X = X[y != 2]\n",
        "binary_y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(binary_X, binary_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using Seaborn's heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix for Logistic Regression\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "It0nMrQVbPD9",
        "outputId": "23bdfe3b-bc29-4c77-8f78-01b2c1033f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q12)Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,Recall, and F1-ScoreM\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "a8MPsNYVbfO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (labels)\n",
        "\n",
        "# To make it a binary classification problem, we'll filter to only include two classes (e.g., class 0 and class 1)\n",
        "binary_X = X[y != 2]\n",
        "binary_y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(binary_X, binary_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20a3TE-Vb0vI",
        "outputId": "d21f3842-596a-4e87-b5eb-bbd203a2a9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q13)Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "6pN1Tas1cEwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Create an imbalanced dataset (e.g., 1000 samples with 95% class 0 and 5% class 1)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                            weights=[0.95, 0.05], random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with class_weight='balanced'\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dki3uS_4pJfC",
        "outputId": "3ba8140e-5853-4c9e-efe3-da67bf01b793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8350\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.85      0.91       189\n",
            "           1       0.18      0.55      0.27        11\n",
            "\n",
            "    accuracy                           0.83       200\n",
            "   macro avg       0.57      0.70      0.59       200\n",
            "weighted avg       0.93      0.83      0.87       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q14)Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "SmQcaqm5pS5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "# Impute missing 'Age' with median and 'Embarked' with mode\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "df['Embarked'] = LabelEncoder().fit_transform(df['Embarked'])\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "id": "tBk2SDkgp8wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q15)Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "X8tWpah1qAjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with scaling\n",
        "model_with_scaling = LogisticRegression(max_iter=200)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling:.4f}\")\n"
      ],
      "metadata": {
        "id": "oTNxEAWdqz88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q16)Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "sn28EEmgq2VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities (for the positive class)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class (class 1)\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot the ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='b', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line (random classifier)\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j358de4GrwD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q17)Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "3B5IIbWjr0V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Model Accuracy with C=0.5: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "qEGhSJKesnpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q18)Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "OZhipLDms1NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Retrieve the model coefficients\n",
        "coefficients = model.coef_[0]\n",
        "feature_names = X.columns\n",
        "\n",
        "# Display the feature importance based on the coefficients\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "feature_importance['Absolute Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "\n",
        "# Sort the features by the absolute coefficient value to identify important features\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Print the most important features\n",
        "print(\"\\nMost important features based on model coefficients:\")\n",
        "print(feature_importance)\n"
      ],
      "metadata": {
        "id": "9zd_BYXitVjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q19)Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "edhoD4XCtYni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate Cohen's Kappa score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Print the Cohen's Kappa score\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "_b5USxkOtynN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q20)Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "CCVI-4Got0r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities (needed for Precision-Recall Curve)\n",
        "y_probs = model.predict_proba(X_test_scaled)[:, 1]  # Probability of positive class (Survived=1)\n",
        "\n",
        "# Calculate precision, recall, and thresholds for different probability cutoffs\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Calculate the AUC of the Precision-Recall curve\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label=f'Precision-Recall Curve (AUC = {pr_auc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Logistic Regression')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qsPzsqL3uu-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q21)Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "dcgzJ4hYu1aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Initialize an empty dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Loop through each solver and train the model\n",
        "for solver in solvers:\n",
        "    # Initialize the Logistic Regression model with the current solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate the accuracy of the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store the accuracy for this solver\n",
        "    results[solver] = accuracy\n",
        "\n",
        "# Print the accuracy of each solver\n",
        "print(\"Accuracy comparison of different solvers:\")\n",
        "for solver, accuracy in results.items():\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "rwqE_NNYvYlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q22)Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "EJpWyWgCvasM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the MCC score\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "id": "2MhE15VfwQHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q23)Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "QjuKqCy4wSEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# 1. Train and evaluate on raw data (without scaling)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_raw = model.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# 2. Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate on standardized data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f\"Accuracy on Raw Data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on Standardized Data: {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "4C-uVfIwxDW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q24)Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "hThFJiLXxFyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {'C': np.logspace(-4, 4, 20)}  # Values for C from 10^-4 to 10^4\n",
        "\n",
        "# Initialize GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Perform GridSearchCV to find the best value of C\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best parameter (C) and the corresponding accuracy\n",
        "print(f\"Best C (Regularization Strength): {grid_search.best_params_['C']}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test set accuracy with the optimal C: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "OONkBeWpx1QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q25)Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "Ans:"
      ],
      "metadata": {
        "id": "wE6sUdYxx3sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib  # For saving and loading the model\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values by imputing them\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # For categorical values (Embarked)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']])\n",
        "imputer = SimpleImputer(strategy='median')  # For numerical values (Age)\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Drop columns that won't be used in the model\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Encode categorical variables (Sex and Embarked)\n",
        "df['Sex'] = pd.get_dummies(df['Sex'], drop_first=True)  # Convert Sex into 1 (male) and 0 (female)\n",
        "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)  # One-hot encoding for Embarked\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the trained model and scaler using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')  # Save the model\n",
        "joblib.dump(scaler, 'scaler.joblib')  # Save the scaler\n",
        "\n",
        "# Load the saved model and scaler\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "loaded_scaler = joblib.load('scaler.joblib')\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "X_test_scaled_loaded = loaded_scaler.transform(X_test)  # Standardize the test data using the loaded scaler\n",
        "y_pred_loaded = loaded_model.predict(X_test_scaled_loaded)\n",
        "\n",
        "# Evaluate the model performance on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred_loaded)\n",
        "print(f\"Test set accuracy using the loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "tRiTd2KSyl9A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}